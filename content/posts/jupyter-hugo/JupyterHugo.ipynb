{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "voluntary-therapy",
   "metadata": {},
   "source": [
    "```\n",
    "title: \"Jupyter Notebooks and Hugo\"\n",
    "date: 2021-05-22\n",
    "```\n",
    "\n",
    "Jupyter is a super nice environment for doing [literate programming](https://en.wikipedia.org/wiki/Literate_programming) and as such it's a natural choice for writing code-heavy articles. I've set up several blogs with [Hugo](https://gohugo.io/) which is nice to write in but only understand markdown by default, but thanks to [`nbconvert`](https://nbconvert.readthedocs.io/en/latest/) it's pretty easy to get the two working together.\n",
    "\n",
    "Inside each notebook I include the Hugo [front matter](https://gohugo.io/content-management/front-matter/) in the first markdown cell inside a markdown code block. This keeps everything neatly in a single file per post:\n",
    "\n",
    "````\n",
    "```\n",
    "title: \"Jupyter Notebooks and Hugo\"\n",
    "date: 2021-05-22\n",
    "```\n",
    "````\n",
    "\n",
    "The Python script below is intended to be run from a Hugo project root directory, it will search for notebook files and run them through `nbconvert`, doing some minor transformations along the way:\n",
    "\n",
    "* remove empty code cells\n",
    "* clean up the generated markdown\n",
    "* install the front matter for Hugo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import pathlib\n",
    "import re\n",
    "import nbconvert\n",
    "\n",
    "from nbconvert.exporters.markdown import MarkdownExporter\n",
    "from traitlets.config import Config\n",
    "from nbconvert.preprocessors import Preprocessor\n",
    "\n",
    "DUPLICATE_NEWLINES_RE = re.compile(r\"\\n\\n\\n+\", flags=re.MULTILINE)\n",
    "ROOT_DIR = pathlib.Path(__file__).parent\n",
    "POSTS_DIR = ROOT_DIR / \"content\" / \"posts\"\n",
    "\n",
    "\n",
    "def remove_duplicate_newlines(content: str) -> str:\n",
    "    return DUPLICATE_NEWLINES_RE.sub(\"\\n\\n\", content)\n",
    "\n",
    "\n",
    "def fix_image_links(content: str) -> str:\n",
    "    # They come from nbconvert looking like '[image](attachment:...)', so just\n",
    "    # remove the 'attachment:' part\n",
    "    return content.replace(\"attachment:\", \"\")\n",
    "\n",
    "\n",
    "def fix_front_matter(content: str) -> str:\n",
    "    # Make front matter use --- instead of ```\n",
    "    content = content.split(\"\\n\")\n",
    "    code_fences = []\n",
    "    for i in range(0, min(10, len(content))):\n",
    "        if content[i].strip() == \"```\":\n",
    "            code_fences.append(i)\n",
    "\n",
    "    if len(code_fences) < 2:\n",
    "        raise RuntimeError(\n",
    "            f\"front matter not found first 10 lines of  in '{notebook_path}'\"\n",
    "        )\n",
    "\n",
    "    for line in code_fences[:2]:\n",
    "        content[line] = content[line].replace(\"```\", \"---\")\n",
    "\n",
    "    return \"\\n\".join(content)\n",
    "\n",
    "\n",
    "class ExtractImages(Preprocessor):\n",
    "    \"\"\"Pull out images from notebook\"\"\"\n",
    "\n",
    "    def preprocess(self, nb, resources):\n",
    "        # Find all attachments\n",
    "        attachments = []\n",
    "        for cell in nb.cells:\n",
    "            if \"attachments\" in cell:\n",
    "                attachments.append(cell[\"attachments\"])\n",
    "\n",
    "        # Write out base64 images as files\n",
    "        for attachment in attachments:\n",
    "            for image_name, img_data in attachment.items():\n",
    "                for img_type, base64_code in img_data.items():\n",
    "                    if img_type == \"image/png\":\n",
    "                        bytes = base64.b64decode(base64_code)\n",
    "                        resources[\"outputs\"][image_name] = bytes\n",
    "\n",
    "        return nb, resources\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    notebook_paths = POSTS_DIR.glob(\"*/*.ipynb\")\n",
    "    c = Config()\n",
    "    c.RegexRemovePreprocessor.patterns = [\"\\s*\\Z\"]\n",
    "    c.MarkdownExporter.preprocessors = [\n",
    "        ExtractImages,\n",
    "        \"nbconvert.preprocessors.RegexRemovePreprocessor\",\n",
    "    ]\n",
    "    exporter = nbconvert.MarkdownExporter(config=c)\n",
    "\n",
    "    for notebook_path in notebook_paths:\n",
    "        with open(notebook_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Export to markdown via nbconvert\n",
    "        md_path = notebook_path.with_suffix(\".md\")\n",
    "        md_content, resources = exporter.from_filename(str(notebook_path))\n",
    "\n",
    "        # Write the images to disk\n",
    "        for filename, content in resources[\"outputs\"].items():\n",
    "            with open(notebook_path.parent / filename, \"wb\") as f:\n",
    "                f.write(content)\n",
    "\n",
    "        # Run some processors over the text\n",
    "        filters = [\n",
    "            fix_image_links,\n",
    "            fix_front_matter,\n",
    "            remove_duplicate_newlines,\n",
    "        ]\n",
    "\n",
    "        for filter in filters:\n",
    "            md_content = filter(md_content)\n",
    "\n",
    "        # Write out the new content\n",
    "        md_path = md_path.parent / \"index.md\"\n",
    "        with open(md_path, \"w\") as f:\n",
    "            f.write(md_content.strip() + \"\\n\")\n",
    "        print(f\"Wrote {md_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-inside",
   "metadata": {},
   "source": [
    "Assuming this in some file such as `my_repo/make_notebooks.py`, you make the notebooks automatically rebuild by watching the source files (requires `entr`, on Ubuntu you can get it via `sudo apt install entr`):\n",
    "\n",
    "```bash\n",
    "find ./content -name \"*.ipynb\" | grep -v checkpoint | entr python make_notebooks.py\n",
    "```\n",
    "\n",
    "Start the development server in another terminal:\n",
    "\n",
    "```bash\n",
    "hugo server -D\n",
    "```\n",
    "\n",
    "Personally I like to jam these all into a `Makefile` like so:\n",
    "\n",
    "```make\n",
    "notebooks_dev:\n",
    "    find ./content -name \"*.ipynb\" | grep -v checkpoint | entr make notebooks\n",
    "   \n",
    "hugo_dev:\n",
    "    hugo server -D\n",
    "    \n",
    "dev: hugo_dev notebooks_dev\n",
    "```\n",
    "\n",
    "and run it with 2 threads (one for each job):\n",
    "\n",
    "```bash\n",
    "make dev -j2\n",
    "```\n",
    "\n",
    "And that's all there is to it! Everything turns into markdown in the end and the computers are all happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
