<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Goatmobile Website</title>
    <link>/posts/</link>
    <description>Recent content in Posts on Goatmobile Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Jupyter Notebooks and Hugo</title>
      <link>/posts/jupyter-hugo/</link>
      <pubDate>Sat, 22 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/jupyter-hugo/</guid>
      <description>Jupyter is a super nice environment for doing literate programming and as such it&amp;rsquo;s a natural choice for writing code-heavy articles. I&amp;rsquo;ve set up several blogs with Hugo which is nice to write in but only understand markdown by default, but thanks to nbconvert it&amp;rsquo;s pretty easy to get the two working together.
Inside each notebook I include the Hugo front matter in the first markdown cell inside a markdown code block.</description>
    </item>
    
    <item>
      <title>Multiple GitHub Users on a Single Machine: The Simple Way</title>
      <link>/posts/multiple-git-users/</link>
      <pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/multiple-git-users/</guid>
      <description>Generate a key.
# Give your keys a special name! For this example it&amp;#39;s &amp;#39;my_cool_key&amp;#39; ssh-keygen -t ed25519 -N &amp;#34;&amp;#34; -C &amp;#34;my key&amp;#39;s name&amp;#34; -f ~/.ssh/my_cool_key   Copy it to whatever GitHub account it should be on.
cat ~/.ssh/my_cool_key.pub   Go to your repository and do some pushes! Use GIT_SSH_COMMAND to tell it to use the key we just generated.
GIT_SSH_COMMAND=&amp;#39;ssh -i ~/.ssh/my_cool_key -o IdentitiesOnly=yes&amp;#39; git push   Wow!</description>
    </item>
    
    <item>
      <title>Scalar Autodiff From Scratch</title>
      <link>/posts/scalar-autodiff/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/scalar-autodiff/</guid>
      <description>Automatic differentiation (or autodiff) underlies many modern machine learning frameworks and mathematical models. This post demonstrates a from-scratch implemention on scalar values (in contrast to tensors a.k.a. ndarrays that are common in machine learning) and shows how we can use it to implement gradient descent, eventually learning some linear function (i.e. a function looks like y = m * x + b). Rest assured the explanations here are much more complicated than the code itself.</description>
    </item>
    
  </channel>
</rss>
